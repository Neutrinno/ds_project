import click
import logging
import pickle
from pathlib import Path
from typing import Any, Dict, Optional

import mlflow
import mlflow.sklearn
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
import yaml  # type: ignore[import-untyped]

from src.data.s3_utils import (
    get_s3_client,
    download_file_from_s3,
    upload_model_to_s3,
)

logger = logging.getLogger(__name__)


def load_config(config_path: Path) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥ –∏–∑ YAML —Ñ–∞–π–ª–∞"""
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    assert isinstance(config, dict), f"Config file {config_path} must contain a dictionary"
    return config  # type: ignore[return-value]


def train_model(
    dataset_path: Path,
    config: Dict[str, Any],
    experiment_name: str,
    mlflow_tracking_uri: str,
    s3_client: Any,
    s3_bucket: str,
    s3_endpoint: str,
    s3_access_key: str,
    s3_secret_key: str,
    dataset_s3_key: Optional[str] = None,
) -> None:
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ –ª–æ–≥–∏—Ä—É–µ—Ç –≤ MLFlow"""

    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω S3 –∫–ª—é—á, —Å–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ S3
    if dataset_s3_key:
        logger.info(f"Downloading dataset from S3: {dataset_s3_key}")
        download_file_from_s3(s3_client, s3_bucket, dataset_s3_key, dataset_path)

    logger.info(f"Loading dataset from {dataset_path}")
    df = pd.read_csv(dataset_path)

    # –£–¥–∞–ª—è–µ–º Id –∫–æ–ª–æ–Ω–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å (—É—Ç–µ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö)
    if "Id" in df.columns:
        df = df.drop(columns=["Id"])
        logger.info("Dropped 'Id' column to prevent data leakage")

    # –ü–æ—Å–ª–µ–¥–Ω—è—è –∫–æ–ª–æ–Ω–∫–∞ - —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    # –î–ª—è Iris: SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm, Species
    target_col = df.columns[-1]
    feature_cols = df.columns[:-1]

    X = df[feature_cols]
    y = df[target_col]

    logger.info(f"Dataset shape: {X.shape}, target: {target_col}")
    logger.info(f"Features: {list(feature_cols)}")

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLFlow
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    mlflow.set_experiment(experiment_name)

    # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    hyperparams = config["hyperparameters"]
    model_type = config.get("model_type", "RandomForestClassifier")

    logger.info(f"Training {model_type} with hyperparameters: {hyperparams}")

    with mlflow.start_run():
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if model_type == "RandomForestClassifier":
            model = RandomForestClassifier(
                n_estimators=hyperparams.get("n_estimators", 100),
                max_depth=hyperparams.get("max_depth", None),
                max_features=hyperparams.get("max_features", "sqrt"),
                random_state=42,
            )
        else:
            raise ValueError(f"Unknown model type: {model_type}")

        # –û–±—É—á–µ–Ω–∏–µ
        model.fit(X_train, y_train)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test)

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        metrics = {
            "accuracy": accuracy_score(y_test, y_pred),
            "precision_macro": precision_score(y_test, y_pred, average="macro"),
            "recall_macro": recall_score(y_test, y_pred, average="macro"),
            "f1_macro": f1_score(y_test, y_pred, average="macro"),
        }

        logger.info(f"Metrics: {metrics}")

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLFlow
        mlflow.log_params(hyperparams)
        mlflow.log_metrics(metrics)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ
        model_dir = Path("models") / experiment_name
        model_dir.mkdir(parents=True, exist_ok=True)
        model_path = model_dir / "model.pkl"

        with open(model_path, "wb") as f:
            pickle.dump(model, f)

        logger.info(f"Model saved to {model_path}")

        try:
            mlflow.sklearn.log_model(model, name="model")
            logger.info("Model logged to MLFlow")
        except Exception as e:
            logger.warning(f"Failed to log model to MLFlow: {e}")
            logger.warning("Continuing without MLFlow model artifact")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ S3
        upload_model_to_s3(
            s3_client,
            s3_bucket,
            model_path,
            experiment_name,
            "model.pkl",
        )

        logger.info(f"Model uploaded to S3: {experiment_name}/model.pkl")

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ run –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
        run = mlflow.active_run()
        if run:
            run_id = run.info.run_id
            experiment_id = run.info.experiment_id
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ —Å localhost –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞
            tracking_uri = mlflow_tracking_uri.replace("mlflow", "localhost")
            run_url = f"{tracking_uri}/#/experiments/{experiment_id}/runs/{run_id}"
            experiment_url = f"{tracking_uri}/#/experiments/{experiment_id}"
            run_name = getattr(run.info, "run_name", run_id)
            print(f"üèÉ View run {run_name} at: {run_url}")
            print(f"üß™ View experiment at: {experiment_url}")


@click.command()
@click.argument("config_path", type=click.Path(exists=True))
@click.argument("dataset_path", type=click.Path())
@click.option("--mlflow-uri", default="http://localhost:5000", help="MLFlow tracking URI")
@click.option("--bucket", default="iris-datasets", help="S3 bucket name")
@click.option("--endpoint", default="http://localhost:9000", help="S3 endpoint URL")
@click.option("--access-key", default="minioadmin", help="S3 access key")
@click.option("--secret-key", default="minioadmin", help="S3 secret key")
@click.option("--dataset-s3-key", default=None, help="S3 key for dataset (if downloading from S3)")
def main(
    config_path: str,
    dataset_path: str,
    mlflow_uri: str,
    bucket: str,
    endpoint: str,
    access_key: str,
    secret_key: str,
    dataset_s3_key: Optional[str],
) -> None:
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É –∏ –ª–æ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ MLFlow"""

    logger.info("Starting model training")

    config = load_config(Path(config_path))
    experiment_name = config.get("experiment_name", "default_experiment")

    s3_client = get_s3_client(endpoint, access_key, secret_key)

    dataset_path_obj = Path(dataset_path)
    # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ –∏ —É–∫–∞–∑–∞–Ω S3 –∫–ª—é—á, —Å–æ–∑–¥–∞–µ–º –ø—É—Ç—å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    if not dataset_path_obj.exists() and dataset_s3_key:
        dataset_path_obj.parent.mkdir(parents=True, exist_ok=True)

    train_model(
        dataset_path=dataset_path_obj,
        config=config,
        experiment_name=experiment_name,
        mlflow_tracking_uri=mlflow_uri,
        s3_client=s3_client,
        s3_bucket=bucket,
        s3_endpoint=endpoint,
        s3_access_key=access_key,
        s3_secret_key=secret_key,
        dataset_s3_key=dataset_s3_key,
    )

    logger.info("Training completed")


if __name__ == "__main__":
    log_fmt = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    logging.basicConfig(level=logging.INFO, format=log_fmt)
    main()
